{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Lab 3(a) - Implementation of Multi layer perceptron from scratch\n",
    "##  Weightage - 2.4%\n",
    "\n",
    "Maximum Points in the Lab: 90\n",
    "\n",
    "---\n",
    "Important points to remember :\n",
    "\n",
    "\n",
    "1.  Observations for the experiments should be explained.\n",
    "2. All the code should be submitted in the form of a single Jupyter notebook itself.\n",
    "3. Points for each sub-section are mentioned in the appropriate question.\n",
    "4. Make sure to begin early as a few experiments may consume more time to run.\n",
    "5. You can use Google colab to run in jupyter notebook (https://colab.research.google.com/) How to load data in Google Colab ?(https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92)\n",
    "6. The lab must be submitted on Google classroom. The code as well as the accompanying observations should be made part of the python notebook.\n",
    "7. **Code Readability** is very important. Hence use self explanatory variable names and add comments to describe your approach wherever necessary.\n",
    "8. You are expected to submit your **detailed inferences** and not just an error free code.\n",
    "9. The lab is due on **March 20th 11.59pm**.\n",
    "10. The lab should be completed **individually**. Students are expected to follow the **honor code** of the class.\n",
    "\n",
    "For any doubts regarding lab please mail to 2018csm1011@iitrpr.ac.in\n",
    "\n",
    "\n",
    "Below is the multi layer perceptron architechture used for implementation. Notations of the neural network are mentioned below :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLP implementation diagram](pictures/mlp.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation details :\n",
    "\n",
    "1) In this lab you will be using MLP for classifying MNIST digits. a. Let us consider a MLP network with H hidden units, O outputs, and\n",
    "inputs of size D.\n",
    "\n",
    "2) According to above diagram dimensions of network will be :\n",
    "```\n",
    " W - H * D+1   -- Weights from Input layer to Hidden layer\n",
    " V - K * H+1   -- Weights from Hidden layer to Output layer\n",
    " X - N * D+1   -- Weights of Input data\n",
    " Y - N * K     -- Weights of Output label data\n",
    " Z - H+1 * 1   -- Hidden layer Weights\n",
    " O - K * 1     -- Output layer weights\n",
    "```\n",
    "3) Please note that +1 in above notations is to indicate bias term.\n",
    "\n",
    "4) tanh is used as the activation function.\n",
    "\n",
    "5) During forward pass will be :\n",
    "\n",
    "$\\mathbf{z}=\\tanh (\\mathbf{W} \\mathbf{x})$\n",
    "\n",
    "and $O_{i}=\\frac{\\exp v_{i}^{T} z}{\\sum_{k=1}^{K} \\exp v_{k}^{T} z}$\n",
    "\n",
    "Overall loss function will be :\n",
    "\n",
    "Total loss = $-\\sum_{n=1}^{N} \\sum_{i=1}^{K} y_{n i} \\log O_{n i}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) The dataset is included in zip file (\"data.txt\" and \"label.txt\"). Number of hidden layer units to be 500, learning rate is set to be 0.01. \n",
    "\n",
    "7) Inorder to update the weights during back propogation we will modified version of stochastic gradient descent, where instead of updating weights after each data point, the updates are made once with batch of input data, Let batch size = 25. Number of epochs = 100.\n",
    "\n",
    "8) Divide the data into train, validation, and test splits using a preset ratio. Please define the ratio you are using.\n",
    "\n",
    "9) Please plot the below : \n",
    "\n",
    "    i)  Training error, Validation error Vs epochs [average over 5 runs.]\n",
    "  \n",
    "    ii) Mean Training error Vs epochs [average over 5 runs.]\n",
    "    \n",
    "    iii)  Mean Validation error Vs epochs [average over 5 runs.]\n",
    "  \n",
    "    iv) Variance Training error Vs epochs [average over 5 runs.]\n",
    "    \n",
    "     v)  Variance validation error Vs epochs [average over 5 runs.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Note : \n",
    "\n",
    "1) All weight update equations during back propogation should be done using $\\textbf{Matrix operations}$ only (not for loops).\n",
    "\n",
    " For example : \n",
    " \n",
    "![MLP implementation diagram](pictures/equation.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum points : 90 points.\n",
    "\n",
    "1) Derive the weight update equation for W in form of Matrix operations similar to V matrix operations defined above.(write in the jupyter notebook itself using Latex or image) - 10 pts\n",
    "\n",
    "2) Splitting train, validation and test using preset ratio - 5 pts\n",
    "\n",
    "3) Random weights assign to W,V values -  5 pts.\n",
    "\n",
    "4) During Forward pass :\n",
    "   \n",
    "       updating Z values - 5 pts.\n",
    "\n",
    "       updating O values - 5 pts.\n",
    "\n",
    "       Applying softmax values - 5pts.\n",
    "\n",
    "       Calculating error values - 5pts.\n",
    "\n",
    "5) During backward pass : ( only matrix operations allowed)\n",
    "   \n",
    "       Gradient between hidden to output - 10 pts.\n",
    "\n",
    "       Gradient betweem input to hidden - 15 pts.\n",
    "   \n",
    "6) Using batch size = 25 to update weights - 5 pts.\n",
    "\n",
    "7) averaging over 5 runs - 5 pts.\n",
    "\n",
    "8) Please plot the below : - 10pts. \n",
    "\n",
    "    i)  Training error, Validation error Vs epochs [average over 5 runs.]\n",
    "  \n",
    "    ii) Mean Training error Vs epochs [average over 5 runs.]\n",
    "    \n",
    "    iii)  Mean Validation error Vs epochs [average over 5 runs.]\n",
    "  \n",
    "    iv) Variance Training error Vs epochs [average over 5 runs.]\n",
    "    \n",
    "     v)  Variance validation error Vs epochs [average over 5 runs.]   \n",
    "     \n",
    "9) Discuss the observations from above plots. - 5pts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "####################################################################################\n",
    "# Derivative of weights as defined in Lab3.a\t####################################\n",
    "####################################################################################\n",
    "####################################################################################\n",
    "\n",
    "# Dimension of all the Matrices Used\n",
    "# W - H * D+1   -- Weights from Input layer to Hidden layer\n",
    "# V - K * H+1   -- Weights from Hidden layer to Output layer\n",
    "# X - N * D+1   -- Weights of Input data\n",
    "# Y - N * K     -- Weights of Output label data\n",
    "# Z - H+1 * 1   -- Hidden layer Weights\n",
    "# O - K * 1     -- Output layer weights\n",
    "\n",
    "\n",
    "# Function to read data and store it in form of 2D array\n",
    "def read_data(file_name) :\n",
    "    data = np.loadtxt(file_name, delimiter=',')\n",
    "    return data\n",
    "\n",
    "\n",
    "# Implementation of Forward pass function\n",
    "def forward_pass(data_points,W,V,Y) :\n",
    "    # use compute_Z_values, compute_O_values, compute_softmax, calculate_error to compute error, O_softmax\n",
    "    # During forward pass compute Z values, O_values, O_softmax, error\n",
    "    # Insert code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #return the error, O_softmax\n",
    "    return error,O_softmax\n",
    "\n",
    "\n",
    "# Implementation of Cross Entropy Error Function takes as input O_softmax, Y\n",
    "def calculate_error(predictions, targets, epsilon=1e-10):\n",
    "    # Caculate cross entropy error between output of softmax (predictions) , actual values (targets)\n",
    "    #Insert code here\n",
    "    \n",
    "\n",
    "    #returns cross entropy error\n",
    "    return cross_entropy_error\n",
    "\n",
    "\n",
    "# Implementation of Softmax Error Function takes as input O\n",
    "def compute_softmax(output_matrix) :\n",
    "    # return output_matrix after apply softmax function ( hint: use np.exp function )\n",
    "    y_length = 10\n",
    "    #Insert code here\n",
    "\n",
    "\n",
    "    # returns output_matrix\n",
    "    return output_matrix\n",
    "\n",
    "\n",
    "# Implementation of Backward pass using Backpropagation Algorithm to calculate V_new, W_new, bias_v\n",
    "def backward_pass(O_softmax,Y,V,Z,W,X,bias_z):\n",
    "    # use gradient_hidden_to_output, gradient_input_to_hidden functions to compute V_new, bias_v, W_new \n",
    "    #Insert code here\n",
    "  \n",
    "\n",
    "    #returns V_new, W_new, bias_v\n",
    "    return  W_new,V_new,bias_v\n",
    "\n",
    "\n",
    "#Implementation of Graident back propogation from Hidden to Input Layer\n",
    "def gradient_hidden_to_output(O_softmax,Y,Z,bias_z) :\n",
    "    # function to update V values using backpropogation using matrix operations.\n",
    "    #Insert code here\n",
    "\n",
    "\n",
    "\n",
    "    return final_result_matrix,bias_v\n",
    "\n",
    "#Implementation of Graident back propogation from Input to Hidden Layer\n",
    "def gradient_input_to_hidden(O_softmax,Y,V,Z,X) :\n",
    "    # function to update W values using backpropogation using only matrix operations.\n",
    "    #Insert code here\n",
    "\n",
    "\n",
    "    # returns updated W values\n",
    "    return result_matrix\n",
    "\n",
    "\n",
    "\n",
    "# Function to calculate Z values during forward pass\n",
    "def compute_Z_values(weights,data_points) :\n",
    "    # function to update Z during forward pass using matrix operations.\n",
    "    #Insert code here\n",
    "\n",
    "\n",
    "    #return calculated z_values\n",
    "    return z_values\n",
    "\n",
    "\n",
    "# Function to Calculate output matrix during forward pass\n",
    "def compute_O_values(weights,z_values) :\n",
    "    # function to update O during forward pass using matrix operations.\n",
    "    #Insert code here\n",
    "\n",
    "\n",
    "    #return calculated o_values\n",
    "    return o_values.T\n",
    "\n",
    "\n",
    "\n",
    "# Function to Intialise weights with bias term\n",
    "def initilaise_weights(data) :\n",
    "    # function to append bias term.\n",
    "    # insert code here\n",
    "\n",
    "\n",
    "    return final_data\n",
    "\n",
    "\n",
    "\n",
    "# To intiliase random weights to Matrices such as W, V\n",
    "def random_weights(number_of_rows,number_of_columns) :\n",
    "    # Function to assign random weights to W, V\n",
    "    #Insert code here \n",
    "    \n",
    "\n",
    "    # return random weights with number_of_rows * number_of_columns\n",
    "    return new_data\n",
    "\n",
    "\n",
    "# To divide the data into test train data\n",
    "def train_test_split(X,Y,fraction) :\n",
    "    # Function to divide train, validation and test data based on fraction. let fraction = 0.8 then train = 0.75, validation= 0.05 and test = 0.2 \n",
    "    #Insert code here \n",
    "    \n",
    "\n",
    "    # return data_train_x,data_train_y,validation_data_x,validation_data_y,test_data_x,test_data_y\n",
    "    return data_train_x,data_train_y,validation_data_x,validation_data_y,test_data_x,test_data_y\n",
    "\n",
    "\n",
    "# Shuffle in same order for X,Y\n",
    "def shuffle(a, b, seed):\n",
    "   # to Shuffle in same order for X,Y based on seed\n",
    "   #Insert code here\n",
    "\n",
    "\n",
    "   # return shuffled values a,b in same order\n",
    "   return a,b\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    data = read_data(\"data.txt\")\n",
    "    Y = read_data(\"label.txt\")\n",
    "    X = initilaise_weights(data)\n",
    "    W = random_weights(500,401)\n",
    "    V = random_weights(10,501)\n",
    "    Z = compute_Z_values(W,X[:25,:])\n",
    "    O = compute_O_values(V,Z)\n",
    "    bias_z = np.empty(shape=(25, 1))\n",
    "    bias_z.fill(1.0)\n",
    "    i=0\n",
    "    learning_rate = 0.01\n",
    "    train_test_fraction  = 0.8\n",
    "    train_validation_split = 0.2\n",
    "    train_data_x,train_data_y,validation_data_x,validation_data_y,test_data_x,test_data_y =train_test_split(X,Y,train_test_fraction)\n",
    "    number_of_epocs=100\n",
    "    train_error_epoch = []*(5*number_of_epocs)\n",
    "    #X = train_data_x\n",
    "    #Y = train_data_y\n",
    "    #Dividing the data into training data and test data into 0.8 ratio same ration for train and validation data,values below correspond to 0.8 ratio\n",
    "    train_data_len = 3200\n",
    "    validation_data_len = 800\n",
    "    test_data_len =1000\n",
    "    validation_error_epoch = [] * (5*number_of_epocs)\n",
    "    # Running for 5 trails using 100 Epocs and Batch size = 25\n",
    "    batch_size = 25\n",
    "    print(\"Started calculating Training error in 5 Trails for each epoch with Batch size = 25 \")\n",
    "    print(\"Started calculating Validation error in 5 Trails for each epoch with Batch size = 25 \")\n",
    "    # Different trails are performed for 5 times.\n",
    "    # 5 different trails\n",
    "    for k in range(5) :\n",
    "        W = random_weights(500, 401)\n",
    "        V = random_weights(10, 501)\n",
    "        error_train=0\n",
    "        error_validation = 0\n",
    "        # Randomising the data\n",
    "        seed = random.randint(10000,10000000)\n",
    "        X,Y = shuffle(X,Y,seed)\n",
    "        print(\"Training Error for 100th epoch for Trail Number : \"+str(k+1))\n",
    "        for j in range(number_of_epocs) :\n",
    "            i=0\n",
    "            count=0\n",
    "            error_train = 0.0\n",
    "            error_validation = 0.0\n",
    "            #X,Y = shuffle(X,Y,12345)\n",
    "            while i < (train_data_len)  :\n",
    "                i1=i\n",
    "                # Batch size is 25\n",
    "                i= i+25\n",
    "                error,O_softmax=forward_pass(X[i1:i, :], W, V, Y[i1:i, :])\n",
    "                W_new,V_new,bias_v=backward_pass(O_softmax,Y[i1:i,:],V,Z,W,X[i1:i,:],bias_z)\n",
    "                #print(W_new.shape)\n",
    "                W = W - (learning_rate/25)*W_new\n",
    "                #print(W)\n",
    "                V_new = np.append(learning_rate*bias_v,V_new,axis=1)\n",
    "                V = V - (learning_rate/25)*V_new\n",
    "                error_train+= error\n",
    "                count+=1\n",
    "                #print(error1)\n",
    "                #print(V.shape)\n",
    "            #print(error)\n",
    "            #print(j)\n",
    "            error, O_softmax = forward_pass(X[0:3200, :], W, V, Y[0 : 3200, :])\n",
    "            error1, O_softmax = forward_pass(X[3200:4000, :], W, V, Y[3200:4000, :])\n",
    "            error_validation = error1\n",
    "            count = train_data_len/batch_size\n",
    "            count1= validation_data_len/batch_size\n",
    "            error_train = error_train/count\n",
    "            print(\"Training error after  epoch : \"+str(j+1)+\" here every batch size = 25\")\n",
    "            print(error_train)\n",
    "            print(\"Validation error after  epoch : \"+str(j+1)+\" here batch size = 25\")\n",
    "            print(error_validation)\n",
    "            train_error_epoch.append(error_train)\n",
    "            validation_error_epoch.append(error_validation)\n",
    "    print(\"\\n\")\n",
    "    print(\"Final Training Errors after 5 trails and 100 Epocs : \")\n",
    "    print(train_error_epoch)\n",
    "    print(\"\\n\")\n",
    "    print(\"Final Validation Errors after 5 trails and 100 Epocs : \")\n",
    "    print(validation_error_epoch)\n",
    "    mean_training = []\n",
    "    variance_training = []\n",
    "    mean_validation = []\n",
    "    variance_validation = []\n",
    "    train_error_epoch = np.reshape(train_error_epoch,(5,number_of_epocs))\n",
    "    validation_error_epoch = np.reshape(validation_error_epoch,(5,number_of_epocs))\n",
    "    mean_training = np.mean(train_error_epoch, axis=0)\n",
    "    mean_validation = np.mean(validation_error_epoch,axis=0)\n",
    "    variance_training = np.var(train_error_epoch,axis=0)\n",
    "    variance_validation = np.var(validation_error_epoch,axis=0)\n",
    "    mean_training = np.reshape(mean_training,(number_of_epocs,))\n",
    "    mean_validation = np.reshape(mean_validation, (number_of_epocs))\n",
    "    variance_training = np.reshape(variance_training,(number_of_epocs,))\n",
    "    variance_validation = np.reshape(variance_validation,(number_of_epocs,))\n",
    "    epochs = []\n",
    "    for i in range(1,number_of_epocs+1) :\n",
    "        epochs.append(i)\n",
    "    print(\"\\n\")\n",
    "    print(\"Plots are started to Generate In Figures Folder : \")\n",
    "    plt.plot(epochs,mean_training, color='red', label='Training')\n",
    "    plt.xlabel(\"Epoch values\")\n",
    "    #plt.title(\"Plot for Training Error Vs Epochs\")\n",
    "    location = \"./figures/lab3.a_TrainingError\" + \".png\"\n",
    "    #plt.savefig(location)\n",
    "    #plt.close()\n",
    "    plt.plot(epochs, mean_validation,color='blue', label='Validation')\n",
    "    plt.ylabel(\"Training,Validation Error values\")\n",
    "    plt.title(\"Plot for Training,Validation Error Vs Epochs\")\n",
    "    plt.legend(loc='best')\n",
    "    location = \"./figures/lab3.a_TrainingAndValidationError\" + \".png\"\n",
    "    plt.savefig(location)\n",
    "    plt.close()\n",
    "    #plt.ylim(0.0145,0.01465)\n",
    "    plt.plot(epochs, mean_training, color='red', label='Training')\n",
    "    plt.xlabel(\"Epoch values\")\n",
    "    plt.ylabel(\"Mean Training Error values\")\n",
    "    plt.title(\"Plot for Mean Training Error Vs Epochs\")\n",
    "    location = \"./figures/lab3.a_MeanTrainingError\" + \".png\"\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(location)\n",
    "    plt.close()\n",
    "    #plt.ylim(0.0133, 0.134)\n",
    "    plt.plot(epochs, mean_validation, color='blue', label='Validation')\n",
    "    plt.xlabel(\"Epoch values\")\n",
    "    plt.ylabel(\" Mean Validation Error values\")\n",
    "    plt.title(\"Plot for Mean Validation Error Vs Epochs\")\n",
    "    location = \"./figures/lab3.a_MeanValidationError\" + \".png\"\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(location)\n",
    "    plt.close()\n",
    "    #plt.ylim(0.000240, 0.000242)\n",
    "    plt.plot(epochs, variance_training, color='red', label='Training')\n",
    "    plt.xlabel(\"Epoch values\")\n",
    "    plt.ylabel(\" Variance Training Error values\")\n",
    "    plt.title(\"Plot for Variance Training Error Vs Epochs\")\n",
    "    location = \"./figures/lab3.a_VarianceTrainingError\" + \".png\"\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(location)\n",
    "    plt.close()\n",
    "    #plt.ylim(0.000210, 0.000211)\n",
    "    plt.plot(epochs, variance_validation, color='blue', label='Validation')\n",
    "    plt.xlabel(\"Epoch values\")\n",
    "    plt.ylabel(\"Variance Validation Error values\")\n",
    "    plt.title(\"Plot for Variance Validation Error Vs Epochs\")\n",
    "    location = \"./figures/lab3.a_VarianceValidationError\" + \".png\"\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(location)\n",
    "    plt.close()\n",
    "    print(\"\\n\")\n",
    "    print(\"Plots are Generated Successfully In Figures folder\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
